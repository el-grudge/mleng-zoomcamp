In week 8Ô∏è‚É£ of the ML Zoomcamp we covered:

üöÄ Deep Learning with TensorFlow + Keras  
We explore Keras, an abstraction layer on top of TensorFlow that facilitates building / training neural networks. We use it to build an image classifier that can identify different a clothing item from an image. Keras comes with a number of pre-trained models and we use the Xception model for our problem, first trying it out-of-the-box, then tuning it to our dataset. We cover important concepts including batching and image preprocessing.

üß† Convolutional Neural Network and Transfer Learning  
We learn the intuition behind CNNs, a feed forward neural network architecture typically used for image classification (one famous implementation of CNNs is [AlexNet](https://en.wikipedia.org/wiki/AlexNet), which won the ImageNet challenge in 2012). CNNs have the following layers:

* Convolution layer: Takes the image and scans it using special matrix-like filters for valuable features, which can be thought of as the basic structures - such as lines and curves - that make up an image. 
* ReLU / Pooling layers: Introduces non-linearity to the network and trims down the dimension of the feature map. Non-linearity allows the network to learn the complex relations between the input (image) and output (class), while feature reduction decreases the complexity of the network to prevent overfitting.
* Dense layer: Connects neurons from previous layers to the output layer where logistic regression is used to classify the image. 

One cool thing about neural networks is that their learning can be transferred; a pretrained model can be used on a task different from the one it was originally trained for. For the fashion classification problem, we use the CONV layer of a pretrained Xception model, taking advantage of the features it learned when originally trained, and re-train the dense layer on the fashion images dataset.

üîÑ Learning Rate, Checkpointing, and More Layers  
We discuss loss functions and how to tune the learning rate hyperparameter to minimize the loss value. We also cover callback functions and how to use them to create a saved copy of the trained model when it reaches certain thresholds. Then, we experiment with adding more layers to the model, a practice that can enhance the accuracy of a neural network.

üîç Dropout and Augmentation  
We learn about the dropout regularization technique, which helps the model resist overfitting by keeping the connections of some active neurons while dropping the rest. We also learn how to introduce more variations and enlarge our training dataset by using different augmentation techniques.

üìö Our homework involved:  

* Building a CNN from scratch. The network had 32 3x3 convolutional filters, followed by a 2x2 pooling layer, and then a dense layer with 64 neurons that is connected to the output layer where the classification is made using the sigmoid function.
* Creating the training and testing datasets using the `ImageDataGenerator` function and training the network on images of bees and wasps.
* Applying various augmentations to the training images and continuing the training of the network using the augmented images.

üëâ The code for this project can be found [here](https://github.com/el-grudge/mleng-zoomcamp/tree/main/week_8). 

#mlzoomcamp #ml_engineering #data_science #learning_in_public #boosting #decision_trees

